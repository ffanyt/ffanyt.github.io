---
title:          "Visual-o1: Understanding ambiguous instructions via multi-modal multi-turn chain-of-thoughts reasoning"
date:           2024-10-4 00:01:00 +0800
selected:       true
pub:            "The International Conference on Learning Representations (ICLR)"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Poster</span>'
pub_date:       "2024"
semantic_scholar_id: f75bb668a1986f24c7a975a50fffc1b546831ef8  # use this to retrieve citation count
abstract: >-
  As large-scale models evolve, language instructions are increasingly utilized in multi-modal tasks. Due to human language habits, these instructions often contain ambiguities in real-world scenarios, necessitating the integration of visual context or common sense for accurate interpretation. However, even highly intelligent large models exhibit significant performance limitations on ambiguous instructions, where weak reasoning abilities of disambiguation can lead to catastrophic errors. To address this issue, this paper proposes Visual-O1, a multi-modal multi-turn chain-of-thought reasoning framework. It simulates human multi-modal multi-turn reasoning, providing instantial experience for highly intelligent models or empirical experience for generally intelligent models to understand ambiguous instructions. Unlike traditional methods that require models to possess high intelligence to understand long texts or perform lengthy complex reasoning, our framework does not significantly increase computational overhead and is more general and effective, even for generally intelligent models. Experiments show that our method not only significantly enhances the performance of models of different intelligence levels on ambiguous instructions but also improves their performance on general datasets. Our work highlights the potential of artificial intelligence to work like humans in real-world scenarios with uncertainty and ambiguity.
cover:          /assets/images/covers/visualo1.png
authors:
  - Minheng Ni
  - Yutao Fan
  - Lei Zhang
  - Wangmeng Zuo
links:
  Paper: https://arxiv.org/pdf/2410.03321
  Code: https://github.com/kodenii/Visual-O1
---
